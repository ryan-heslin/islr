


# Matrix Algebra

It is sometimes better to keep predictors in a matrix and outcomes in a separate vector.
```{r}
library(tidyverse)
library(dslabs)
if(!exists("mnist")) mnist <- read_mnist()
str(mnist)
```

We limit ourselves to the first 1000 predictors and responses.


The approach is:

1. Determine if some digits get more ink than others.
2. Drop pixels (predictors) with little variation, since they explain nothing.
3. Find a cutoff value for unwritten space and set anything below ti to 9
4. Convert entries to 1 (writing) or 0 (no writing).
5. Scale all predictors to common average and SD.
```{r}
x <- mnist$train$images[1:1000,] 
y <- mnist$train$labels[1:1000]
```


Notational note: vectors can be written $X = (x_{1,1},\dots{x_{N,1}})^T$)

Matrix will convert vectors to matrices by filling desired rows and columns. Use b


```{r}
matrix(letters, 13,2)
```
Beware: this receycles without warning if the vector length does not conform
```{r}
matrix(letters, 12,3)
```
Let's take the third row (with 784 predictors) and turn it into a $28\times{28}$ matrix (the square root of the 784).
```{r}
grid <- matrix(x[3,], 28, 28)
```


We can use image to plot these. The first two args are just for axis scales.
```{r}
image(1:28, 1:28, grid)
image(1:28, 1:28, grid[, 28:1])

```

## Row and Column Summaries

Now comapre row averages for each response value. Row averages represent the amount of ink used for each digit.

```{r}
sums <- rowSums(x)
avg <- rowMeans(x)

tibble(labels = as.factor(y), row_averages = avg) %>% 
  qplot(labels, row_averages, data = ., geom = "boxplot") 
```
TO drop predictors that don't vary much, we should take column SDs 
```{r}
sds <- apply(x, 2, sd)
qplot(sds, bins = "30", color = I("black"))
```
Turn the SDs vector into a matrix and plot to reveal the corners don't vary much
```{r}
image(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])
```
Easy enough to filter. Remember to beware drop = TRUE!
```{r}
new_x <- x[ ,sds > 60 ]
dim(new_x)
```

To plot a histogram, turn the matrix into avector. You really should just use as.vector.
```{r}
qplot(new_x[1:length(new_x)])
```

The huge number of zeroes indicates smudges wrongly coded as ink. Remmeber logical subsetting and assignment!

```{r}
new_x[new_x <50] <- 0
```

Now binarize by setting values above the threshold to 1
```{r}
bin_x <- x
bin_x[bin_x < 255/2] <- 0 
bin_x[bin_x > 255/2] <- 1
```



## Matrix Algebra and  Vectorization

Adding and subtracting matrices from/to vectors recycles each vextor element to the correponding matrix row. You could scale rowwise like this:

```{r}
(x - rowMeans(x)) / apply(x, 1, sd)
```
You can get around this by transposing and untransposing. Though better to jsut use sweep,w hich will actually subtract every vector element from the corresponding row (MARGIN = 1) or column. Sweep also lets you define the math operation.

They appear not to have heard os cale, which does this automatically.
```{r}
test1 <- t(t(x) - colMeans(x))

X_mean_0 <- sweep(x, 2, colMeans(x))
x_standardized <- sweep(X_mean_0, 2, colSds(x), FUN = "/")
```

Exercise: get proportions of pixels between 50 and 205. Two approaches:

```{r}
train <- mnist$train$images

dat <- cbind(rowMeans(train > 50 & train < 205), mnist$train$labels) %>% 
  {tapply(X =.[,1], INDEX = list(.[,2]), FUN = mean)}
 

cbind(rowMeans(train > 50 & train < 205), mnist$train$labels) %>% 
  {split(.[,1], .[,2])} %>% 
  map_dbl(mean) %>% 
  enframe()

```


# Distance

You know Euclidean distance.

Let's sample some 2s and 7s

```{r}
library(tidyverse)
library(dslabs)

if(!exists("mnist")) mnist <- read_mnist()

set.seed(1995)
ind <- which(mnist$train$labels %in% c(2,7)) %>% sample(500)
x <- mnist$train$images[ind,]
y <- mnist$train$labels[ind]
```

These are a 7, 2, adn 7. The same numbers are of course closer to each other.

```{r}
x_1 <- x[1,]
x_2 <- x[2,]
x_3 <- x[3,]

sqrt(sum((x_1 - x_2)^2))

sqrt(sum((x_1 - x_3)^2))
#> [1] 2311
sqrt(sum((x_2 - x_3)^2))
```


So much easier with matrix algebra
```{r}
sqrt(crossprod(x_1 - x_2))
```
dist does this at once for every row. Dist objects are used ins everal base functions.
```{r}
d <- dist(x)
```


Plot distance, ordered by digit. Sevens are now toward the ends of the scale, since the order goes (2,7).
```{r}
d_mat <- as.matrix(d)

image(d_mat)
image(as.matrix(d)[order(y), order(y)])
```

The predictor space here consists of all possible $||784||$ vectors that are relevent to the problem. Each represents a possible point corresponding to a configuration of the predictor values Parittioning this space by distance is used by many algrothms.

\[dist(1,2)=\sqrt{\sum_{i=1}^N(x_{i,1}-x_{i,2})}\]

or the square root of the sum of the squared distances of each pair of observations for each predictor.

Just transpose and use dist to compute all pairwise predictor distances


```{r}
d <- dist(t(x))
dim(as.matrix(d))
```


Exercise
```{r}
data("tissue_gene_expression")
dat <- tissue_gene_expression
table(dat$y)

d2 <- dist(dat$x)

image(as.matrix(d2))
```


# Dimension Reduction

Let's do some PCA

Imagine we want to reduce this 2D dataset to 1 dimension. These are heights of twins, some children and somea dults. Not surprisingly, they strongly correlate:
```{r}
set.seed(1988)
library(MASS)
n <- 100
Sigma <- matrix(c(9, 9 * 0.9, 9 * 0.92, 9 * 1), 2, 2)
x <- rbind(mvrnorm(n / 2, c(69, 69), Sigma),
           mvrnorm(n / 2, c(55, 55), Sigma))
```


Some points, naturally, are closer than others. 
```{r}
plot(x)

d <- dist(x)
as.matrix(d)[1, 2]

as.matrix(d)[2, 51]
```

If we naiively drop a predictor, distances obviously fall, since we are now longer adding a value to the sum of sqaures.
```{r}
z <- x[,1]

#plot(d, z)
```

We can approximate pretty well by dividing each observation's distance between the predictors by $\sqrt{2}$ (or, equivalently, averaging the distances of the full dataset before taking the square root), the original number of dimensions, then taking the distances of those transformed values. Here the SD gives average 

\[\sqrt{\frac{1}{2}\sum_{j=1}^2(X_{1,j}-X_{2,j})^2}\]

```{r}
sd(dist(x) - dist(z)*sqrt(2))
```

If we compare the average of the summed points to the difference of the vectors, they have almost no relationship.
The distance between any two points  is the line connecting them, and in this highly correlated dataset _those lines mostly parallel the diagonal_. If we plot the difference of the two predictors against the average distance, almost no correaltion remains. We have reduced a dimension and lost almost no information:

Note we haven't applied the square root transformation yet.
```{r}
z  <- cbind((x[,2] + x[,1])/2,  x[,2] - x[,1])
plot(z)
```
If we compute the distance for the averaged data, the approximation improves further. We multiply by $sqrt2$ here to restore the dimension we dropped:
```{r}
sd(dist(x) - dist(z[,1])*sqrt(2))

plot(dist(x), dist(z[,1]*sqrt(2)))
```

To create $Z$, we used the matrix

\[Z = XA\\A = \begin{bmatrix}1/2&1\\
1/2&-1\end{bmatrix}\]

So the first dimension of $Z$ consists of the average of the two predictors, and the second the difference of the two (purely as a comparison). This is of course invertibale. _Then_ we divided $Z$ by $\sqrt2$ to account for losing a dimension. 

We could guarantee no loss of distance across the transformation by scaling the transformation matrix $A$ so that each column's sum of squares is 1 (unit length) and they have no correlation (i.e., are orthogonal and have dot product 0). Then divide by $sqrt2$ to compensate for the lost dimension. For a standardized vector, the sum of sqaures is just the variance. Now any two points in $Z$ have exactly the same distance as in $X$:


```{r}
z2 <- cbind(rep(0, 100), rep(0,100))
z2[,1] <- (x[,1] + x[,2]) / sqrt(2)
z2[,2] <- (x[,2] - x[,1]) / sqrt(2)
max(dist(z2) - dist(x))
```
And the approximation remains good. This is an orthogonal rotation that preserves distances between rows
```{r}
sd(dist(x) - dist(z2[,1]))
```


In the one-dimensional projection, there are still two distinct groups in the data, just as in the original:
```{r}
qplot(z[,1], bins = 20, color = I("black"))
```


Since total variability is the sum of squares in each column, we can confirm that the two components extrcacted every bit of it. With this method, we are rotating the data, so nothing is lost.
```{r}
sum(colMeans(x^2))
#> [1] 7806
sum(colMeans(z2^2))
```
Almost all the varaince is contained by the first component, due to the extreme correlation.

The actual rotation matrix comes very close to the $1/sqrt2$ approximation we ended up using, sicne the predictors wre so highly correlated. Interestingly, prcomp flips the signs of the eigenvectors.
```{r}
prcomp(x)$rotation
1/sqrt(2)

eigen(cor(x))
```


These are all the same, since orthogonal matrices have the inverse as the transpose (ie.e, you could use solve instead of t):
```{r}
a <- sweep(x, 2, colMeans(x)) 
b <- pca$x %*% t(pca$rotation)
max(abs(a - b))


a <- sweep(x, 2, colMeans(x)) %*% pca$rotation
b <- pca$x 
max(abs(a - b))
```

We can imagine each component as averaging the distance between points after extracting the previous one. The more similar the values of each predictor, the more accurate this average is as a sumamry of the original. If red is positive and blue neggative, the one numebr of summary of blue-red is less accurate than that of red-red or blue-blue, jsut as the mean is a better summary the less varaince exists in the data.


## PCA Example: Iris. The three species are visible, with one obviously very distinct from the others.

```{r}
x <- iris[,1:4] %>% as.matrix()
d <- dist(x)
image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "RdBu")))
```

High correlation would reccomend PCA. 2 components would be best, since there are two pairs of highly correlated variables
```{r}
cor(x)
```

```{r}
prcomp(x)$rotation
pca <- prcomp(x)
```

In this example, sepal length, petal length, and petal width "point" in one direction, sepal width in the other (hence its negative correlations with the others). The seond pattern is sepal length and petal width in one direction, petal length and width in the other, with the first pattern being far more important. 


```{r}
eigen(cov(x))$vectors
```

Notice how the projection of points onto the PC axes clearly distingusihes species. The pariwise distances aren't quite exact, but close enough.
```{r}
data.frame(pca$x[,1:2], Species=iris$Species) %>% 
  ggplot(aes(PC1,PC2, fill = Species))+
  geom_point(cex=3, pch=21) +
  coord_fixed(ratio = 1)
```

```{r}
d_approx <- dist(pca$x[, 1:2])
qplot(d, d_approx) + geom_abline(color="red")
```



For the MNIST data, we'd expect that nearby pixels would be correlted, so reduction should be possible. The scree plot shows only a few PCs account for a lot of variability.


```{r}
col_means <- colMeans(mnist$test$images)
pca <- prcomp(mnist$train$images)

pc <- 1:ncol(mnist$test$images)
qplot(pc, pca$sdev)
```


It appears 36 compoenents is a good number. Let's use a knn with that many dimensions.
```{r}
library(caret)
k <- 36
x_train <- pca$x[,1:k]
y <- factor(mnist$train$labels)
fit <- knn3(x_train, y)

x_test <- sweep(mnist$test$images, 2, col_means) %*% pca$rotation
x_test <- x_test[,1:k]

y_hat <- predict(fit, x_test, type = "class")
confusionMatrix(y_hat, factor(mnist$test$labels))$overall["Accuracy"]
```



```{r}
dat <- tissue_gene_expression

pc <- prcomp(tissue_gene_expression$x, scale. = TRUE)$x

data.frame(type = dat$y, pc1 = pc[,1], pc2 = pc[,2]) %>% 
  ggplot(aes(x = pc1, y = pc2, col = type)) +
  geom_point()+
  lims(x = range(pc[,1]), y = range(pc[,2]))
```

Correlation of row means with PC1 score suggests observational bias. It would be a lot worse if I hadn't scaled the data first.
```{r}
tibble(pc1 = pc[,1], means = dat$x %>% rowMeans(), type = dat$y) %>% 
  ggplot(aes(x = pc1, y = means))+
  geom_smooth() +
  geom_point(aes(col= type)) 
 
```


## Sparse Data: Movie Ratings

The movie ratings data has one row per user rating per movie. It is full of implicit NA, since most users did not rate every movie. completing the dataset would crash R. What to do?

A complete matrix would be the Cartesian product of distinct userids and movies.

```{r}
data("movielens")

movielens %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```

We want to predict how users would rate movies that appeared in the daaset but that they did not raet - filling in NA's. This means each prediction has distinct predictors - ratings for that movie by ohter users, and ratings for other moviews by that user. If we use similar users and moviews, the whole dataset can be used to predict!

```{r}


#dat %>% ggplot(aes(x = fct_infreq(factor(title))))+geom_bar()
```


Create partitioned data. Semi join to ensure sets have entirely common elements
```{r}
set.seed(755)
library(caret)
library(dslabs)
data("movielens")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

We define RMSSE as the square root of the average of the sum of prediction errors for all represented movie-user combinations.

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

Create a null model. This is also the SD of the variable, of course, which is average error.
```{r}
mu_hat <- mean(train_set$rating)

naive <- RMSE(train_set$rating, mu_hat)

rmse_results <- tibble(method = "Just the average", RMSE = naive_rmse)
```

Start by adding a fixed effect for movie type, since some receive inherently different ratings. Lm would take forever to do this, so instead just find the mean differences ourselves:

```{r}
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```

This improves RMSE
```{r}
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
RMSE(predicted_ratings, test_set$rating)
```


```{r}

movie_titles <- movielens %>% 
  dplyr::select(movieId, title) %>%
  distinct()

train_set %>% group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n() >= 100) %>% 
   ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
```

We should also add use effects accounting for the movie effect we identified earlier. This makes sense because the effects should cancel or amplify each other, depdning on direction: a bad movie rated by a lenient user gets a good rating.

```{r}

user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

```


Then just attach the effects to each observation and combine to get predictions

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
RMSE(predicted_ratings, test_set$rating)
```

# REgularization

The 10 best- and worst-rated movies all seem obscure (not really), as do
```{r}
test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(residual = rating - (mu + b_i)) %>%
  arrange(desc(abs(residual))) %>%  
  slice(1:10) %>% 
  pull(title)
```

REgularization is a solution to this problem. Without constraining effect sizes, the SSE equation treats an average of 100 ratings the same as a movie with just one rating, even though the latter is far less reliable. The solution is to add a penalty to the SSE for that sums the squares of each estiamte of $b_i$, the user effect:

\[\sum(y_{u,i}-\mu-b_i)^2 + \lambda\sum_ib^2_i\]


The minimizaer,a ccording to calculus, is:

\[\hat{b}_i(\lambda)=\frac{1}{\lambda+n_i}\sum_{u=1}^n_i(Y_{u,i}-\hat{\mu})\]

The penalty adds up for every movie that gets its own effect value ($b_i$). The model compensates by classifying a smaller value of $b_i$ (analogous to ridge regression) for movies with fewer ratings. If $n_i$ (the number of ratings) is large, $\lambda$ becomes irrelevant by comparison, so a larger movie effect is fitted. 


Consider $\lambda = 3$:

```{r}
lambda <- 3
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 
```

And compare to teh original estiamtes

```{r}
tibble(original = movie_avgs$b_i, 
       regularlized = movie_reg_avgs$b_i, 
       n = movie_reg_avgs$n_i) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)
```

Cross-validation (which should of course be done) shows an optiaml lambda is 3.


```{r}
lambdas <- seq(0, 10, 0.25)

mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
#> `summarise()` ungrouping output (override with `.groups` argument)

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
lambdas[which.min(rmses)]
```

We can add multiple effect terms to the lambda term, penalizing both movies with few ratings and users who rate few movies (for the user effect).

EPnalizing both effects yields a further improvement on RMSE:

\[\sum_{u,i}(y_{u,i}-\mu-b_i-b_u)^2+\lambda\Big(\sum_ib_i^2+\sum_ub_u^2\Big)\]

After summing the SSE, sum the squares of every movie and user effect, then multiply by $\lambda$. 

# Matrix Factorization

Our movie rating model ignores the fact that certains groups of users and movies have similar rating patterns. To illustrate this, create a matrix where rows are users, movies columns, and cells the ratings. Most cells are NA's reflecting nonexistent combinations

```{r}
train_small <- movielens %>% 
  group_by(movieId) %>%
  filter(n() >= 50 | movieId == 3252) %>% ungroup() %>% 
  group_by(userId) %>%
  filter(n() >= 50) %>% ungroup()

y <- train_small %>% 
  dplyr::select(userId, movieId, rating) %>%
  spread(movieId, rating) %>%
  as.matrix()
```


Add row and column names.
```{r}
rownames(y)<- y[,1]
y <- y[,-1]

movie_titles <- movielens %>% 
  dplyr::select(movieId, title) %>%
  distinct()

colnames(y) <- with(movie_titles, title[match(colnames(y), movieId)])
```

Sweeping out row and column means leaves residuals, since the means are row and column effects:
```{r}
y <- sweep(y, 2, colMeans(y, na.rm=TRUE))
y <- sweep(y, 1, rowMeans(y, na.rm=TRUE))
```

It would be nice if the residuals were uncorrelated. We are not so lucky.

```{r}
m_1 <- "Godfather, The"
m_2 <- "Godfather: Part II, The"
p1 <- qplot(y[ ,m_1], y[,m_2], xlab = m_1, ylab = m_2)
p1

m_1 <- "Godfather, The"
m_3 <- "Goodfellas"
p2 <- qplot(y[ ,m_1], y[,m_3], xlab = m_1, ylab = m_3)

m_4 <- "You've Got Mail" 
m_5 <- "Sleepless in Seattle" 
p3 <- qplot(y[ ,m_4], y[,m_5], xlab = m_4, ylab = m_5)
```

The correlation is obvious, but how to model it?
```{r}
x <- y[, c(m_1, m_2, m_3, m_4, m_5)]
short_names <- c("Godfather", "Godfather2", "Goodfellas",
                 "You've Got", "Sleepless")
colnames(x) <- short_names
cor(x, use="pairwise.complete")
```
Let's review the correlation of the movies we picked out. We can caputre most of the information in just a few vectors: 
```{r}
x[complete.cases(x)]

q <- c(1,1,1,-1,-1)
p <- matrix(c(2,2,2,0,0,0,0,0,-2,-2,-2,-2), ncol = 1)

p %*% q
```
Moveis are classifed as "gangster" (1) or "romance" (-1). Users are classified as liking gansgter but not romance (2) liking romance and disliking gangster (-2), and indeifferent (0). These weights roughly identify the directions of correlation in the data. _PCA does this exactly by projecting onto the eigenvectors_. From these 17 values, we can almost reconstruct the 6 values of residuals as:

\[r_{u,i}\approx{p_uq_i}\\
R = p^Tq\]

Any $n\times{p}$ where $p<N$ may be decomposed:

\[Y=UDV^T\]
where $U$ and $V$ are orthogoanl matrices of $N\times{p}$ and $p\times{p}$, respectively, adn $D$ a $p\times{p}$ diagonal matrix where the values of the diagonals decrease.

So we should add a $p_uq_i$ term to the regression model. Unfortuantely, the data has far more factors tahn that. If we added _Scent of a Woman_ to the residuals matrix, we would need to add another factor for liking or disliking Al Pacino.

Insert relevant TOny Montana quote here.


```{r}
m_6 <- "Scent of a Woman"
six_movies <- c(m_1, m_2, m_3, m_4, m_5, m_6)
x <- y[, six_movies]
#colnames(x) <- colnames(r)
cor(x, use="pairwise.complete")
```

The two-factor model now needs 36 paramters (from the 6x6 cormat) to explain variability in the 72 ratings. In a real model, we would use continuous values (of a factor). 

## COnnection to Singular Value Decomposition

SVD allows us to decompose $R$ as $R = p^Tq$, so long as the $p$s are uncorrleated. The $qs$ are just PCs, as you should know by now. In this example, we are projecting user effects (the rows) onto movie effects (the columns).


```{r}
y[is.na(y)] <- 0
pca <- prcomp(y)
dim(pca$rotation)
dim(pca$x)

```

The two PC's seem to identify two hidden axes in the data: critically acclaimed-blockbuster, and artsy-nerd movie


SVD Exercise

Consider a dataset with mean-centered grades for 100 students in 24 different subjects. Average grade 0 is a C.
```{r}
set.seed(1987)
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) +
  matrix(rnorm(matrix(n * k * 3)), n, k * 3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))
```

