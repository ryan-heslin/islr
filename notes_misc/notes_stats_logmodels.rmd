

It's for modeling an _exponential_ relationship, silly!
The natural log's usefulness derives from e's definition as $\lim_{x\rightarrow}(1 + \frac{1}{x})^x$). For this reason, natural logs approximate percent changes: for small $\Delta_x$, the difference of the natural logs of $x$ added to the change and x is roughly the change divided by $x$ (the percent change):

\[\ln(x +\Delta_x)-\ln(x)=\frac{\Delta_x}{x}\]

This follow naturally from $e^x$ being its own derivative. The difference of the logs can be written $\ln(\frac{x+\Delta_x}{x})$, and the derivative of this is just the percent change. 


## Interpreting Coefficients:
With the formula

\[P(x)=e^{\beta_0+\beta{x_1}+\dots+\beta{x_n}}\]
, $p(x)$ is constrained to $[0,1]$ That mean the beta expression is constrained by $(-\infty, 0)$. In this case the signs of the coefficients have the usual meaning.

For a logged predictor, the interpretation changes depending on whether $X>1$ (it cannot be negative). If $X<1$, 

## Types of Log Model

### 1. Logged X, Unlogged Y (Linear-Log)

Proportional Change in $X$ Causes Linear Change in $Y$
\[Y_i=\beta_0+\beta_{1}\ln(X_1)+\epsilon\]
In this case, a 1% change (* 1.01) in $X$ causes a $.01\beta_1$ increase in $Y$. This exhibits the key feature of log models: _multiplicative changes now cause linear changes_. To get differences in predicted values, multiply the coef by the difference in logged values:

\[\beta_{1}\times\ln\frac{x_i}{x_j}\\]

### Logged Y, Unlogged X (Log-Linear)

Linear change in $X$ Causes Constant Change in $Y$

In this case, a one-unit change in $X$ causes a $100\times\beta_1$% change in $Y$. This follows from the fact that the natural log approximates a percent change. This is the reverse of the prior case: now linear increases cause multiplicative changes.
\[\ln(\frac{Y +\Delta_Y}{Y})=\beta_0 +\beta_1X+\epsilon\]


## Both $X$ and $Y$ Logged (Log-Log)

Proportional Change in $X$ Causes Proportional Change in $Y$

This form converts the coefficients to proportionality multiplies: a 1% change in $X$ now causes a $\beta_1$% change in $Y$. In other words, $beta_1$ is the elasticity of $Y$ with respect to $X$: the proportion of the change in $Y$ to a change in $X$ or approximately:

\[\frac{\Delta{Y}}{Y}=\beta_1\frac{\Delta{X}}{X}\]

### Note on R-Squared

$R^2$ is appropriate for comparing log-linear and log-log models, as well as linear-log and linear. It cannot meaningfully compare linear-log to log-log models because the dependent variable in the latter has been transformed, so comparisons of variance explained becomes nonsensical.

### Predicting $Y$ after logging

If $Y$ is logged, the fitted values are of $\ln(Y)$, NOT Y itself. TO reverse the logging, just raise the fitted values to the $e$th

\[Y_i=e^{\beta_0+\beta_1x_i +\epsilon}e^{\epsilon_i}\]

Unfortunately, the expected value of $e^{\epsilon_i}$ is not 1, meaning we can't just multiply that term by the rest of the expression. There are several complicated ways to estimate $E(e^{\epsilon_i}|x_i)$ to handle this issue. Best just to leave in log units.

Polynomial predictors can also be logged!

```{r}
library(stringr)
str_rev <- function(str){
  str_sub(str, start =  seq(str_length(str), 1), end = seq(str_length(str), 1)) %>% 
    paste(collapse = "")
}

str_rev("world")
```

