---
title: "ISLR Chapter 6 Notes"
author: "Ryan Heslin"
date: "11/16/2020"
output: pdf_document
---

```{r}
library(tidyverse)
library(rlang)
```

## Review Questions

1. What are the three approaches to subset selection? How do they differ?

2. Why is $R^2$ an inadequate criterion of model performance?

3. What indirect criteria are available to assess test performance?

4. What is the rationale for shrinkage methods?

5. How do the lasso and ridge regression differ? How may each be viewed as an optimization problem?

6. How does principal components regression work?

7. How are principal components constructed?

8. What is the curse of dimensionality?

## Rationales for Rejecting OLS

Several reasons exist to use non-OLS regression:

1. Prediction accuracy. OLS is variant unless $n >> p$ - there are many more observations than predictors. Constraining coefficients will reduce bias in this case.  

2. Interpre. Models with irrelevant variables are hard to interpret; eliminating them simpligies the equation, but OLS almost never sets betas at 0.

The three main methods are subset selection, shrinkage, and dimnesion reduction.



## Subset Selection

To truly find the best subset, we would have to fit models for every proper predictor combination - a prohibitive $2^p$ models. 

The usual algorithm is to fit all possible models with $k$ predictors from $k=1$ to $k=p$, select a best model for each $k$ ($M_{k}$) by lowest SSE, then select the best $M_k$ by cross-validated error or another criterion. 

Importantly, SSE and $R^2$ alone cannot be used to choose the best $M_k$, since they always improve as predictors are added. These statistics tell us only about training error, not test error.

Notice how $R^2$ increases in the graph.

```{r}

forms <- accumulate(colnames(mtcars)[-1], paste, sep = " + ") %>% 
  map(~paste("mpg ~", .x)) %>% 
  map(as.formula) %>% 
  set_names(seq_along(.))

mods <- map(forms, ~lm(data = mtcars, formula = expr(!!.x))) %>% map(broom::glance)

tibble( rsqd = mods %>% map_dbl("r.squared"), preds = seq_along(mods)) %>% 
  ggplot(aes(x = preds, y = rsqd)) +
  geom_line(col = "red")

```

This kind of subset selection is computationally expensive and infeasible past $p=40$. Many predictors also increases the odds of finding a model with low test error rate by coincidence.

###  Forward Stepwise Selection

This approach starts with the null model, then sequentially adds the single predictor that  best improves the fit until a cutoff point is reached

1. For $0 < k <= p$:

a. Start with null model $M_0$  
b. Fit all $p-k$ models, each adding a single predictor.
c. Choose the best as $M_{k+1}$
d. Select best $M_k$ by cross-validation.

This gives a total of just $\frac{1+p(p+1)}{2}$ models. However, it is not guaranteed to find the "best" model, since each additional predictor only considers improvement of the previous iteration's model, locking in the model chosen at the first step; the best model, for instance, may not contain the predictor that yields the best single-predictor model.

### Backward Stepwise Selection

This approach works in the opposite direction, starting with a full model and dropping whichever predictor gives the best model when removed. Unlike forward selection, it requires $ n >p$ (since the full model cannot have more than $n$ predictors).

Several hybrid approaches drop variables that give no improvement at each step.

# Choosing the Best Model
As noted, SSE and $R^2$ do not provide meaningful comparisons among models with differing numbers of predictors. To get around this, we need to estimate test error either indirectly or directly.

## Indirect Approaches

These techniques adjust training error for model size. 

### Cp

For a model with $d$ predictors, the equation
\[C_p=\frac{1}{n}(\text{SSE}+2d\hat\sigma^2)\]

where $\hat\sigma^2$ estimates the error variance of the _full_ (all predictors) model. This essentially charges the model twice the MSE for each added predictor. Predictors improve the criterion if their reduction to SSE outweighs the penalty. 

The denominators here essentially represent the expected value of unexplained variance.

### AIC

The Aikake Information Criterion applies to all models fit by maximum likelihood, which OLS essentially is. The equation:


\[\text{AIC}=\frac{1}{n\hat\sigma^2}(\text{SSE}+2d\hat\sigma^2)\]

and is just $C_p$ divided by MSE.

### BIC

The Bayesian information criterion is:

\[\text{BIC}=\frac{1}{n\hat\sigma^2}(\text{SSE}+log(n)d\hat\sigma^2) \]

Since $log(n) >2$ for any n above 7, the BIC is stricter than $C_p$ for models with $d>7$. 

### Adjusted $R^2$

\[\text{Adj. }R^2 = 1 - \frac{\frac{\text{SSE}}{(n-d-1)}}{\frac{\text{TSS}}{(n-1)}})\]

THis is the standard SSE equation, except with $d$ subtracted from the denominator of the SSE term. The output of this equation shrinks as predictors are added, so unlike $R^2$ bigger values indicate smaller test error. As irrelevant predictors are added, the SSE in the numerator of the upper subfraction increases marginally, but is outweighed by the addition of 1 to the denominator. 

## Direct Estimation

Cross-validation and validation sets may be used to simulate test error. If several models have similar MSE, we pick whichever model within one SE of the minimum of the error curve has the lowest $p$. 

This graph compares these criteria for several models. It seems clear that adding the fifth predictor, wt, gives the most improvement.

```{r}
mods %>% map(~select(.x, c(adj.r.squared, AIC, BIC))) %>% 
  bind_rows(.id = "Preds") %>% 
  mutate(Preds = as.numeric(Preds)) %>% 
  pivot_longer(-Preds, names_to = "Stat") %>% 
  ggplot(aes(x = Preds, y = value, col = Stat)) +
  geom_point()+
  geom_line()+
  facet_wrap(Stat ~ ., scales = "free_y") +
  labs(title = "Models on mtcars") +
  theme(legend.position = "bottom")
  
```

# Shrinkage Methods

Instead of individually dropping or adding predictors, we could fit all of them but shrink some coefficients to 0 or near 0, effectively dropping them. Shrinking coefficient estimates reduces variance, which is high when there are many predictors.

## Ridge Regression

You remember, surely, that

\[\text{SSE}=\sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_i)^2\]

Ridge regression adds a minimization constraint to the SSE formula, to wit:

\[\text{SSE}=\sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_i)^2 +\lambda\sum^p_{j=1}\beta^2_i\]

$\lambda$ is a tuning parameter that scales the sum of squared beta  coefficients. The closer the betas are to 0, the less they increase this term. The betas approach but never reach 0 (except if $\lambda$ is infinite) because they still need to be nonzero to minimize SSE. Higher values of lambda steepen the penalty, reducing beta estimates toward zero. The estimated betas vary for each $\lambda$. Note we don't care about shrinking $\beta_0$, since this represents the mean of the response where all $x_i=\overline{x}_i$ (or 0, if the data are mean-centered) and has no relationship to the association of the response and predictors.

As $\lambda$ increases, the values of coefficients tend to decrease at different rates, though individual betas may increase over some ranges of $\lambda$. Increasing lambda also lowers the $l2$ norm of the betas (or the magnitude - the square root of the summed squares, indicating distance from 0). 

An important caution: standard OLS betas are scale equivariant: multiplying $X$ by $C$ just scales each by $1/C$. This does not apply to ridge regression, since the squaring of the betas is a nonlinear transformation. Ridge regression should therefore _never be done without standardizing_. 

### How Ridge Regression Improves OLS

AS $\lambda$ rises, the fit becomes less flexible, reducing variance at the expense of bias. For a model with many predictors, reducing the variance reduction is welcome. For values of $\lambda$ up to about 10, the bias increase is trivial, leading to a substantial reduction in MSE before increasing bias worsens the fit. In linear relationships, OLS estimates tend to have low bias but high variance (i.e., high likely test error), especially with many predictors. Ridge regression is also computationally cheap.


## The Lasso

Ridge regression does not actually drop any predictors unless $\lambda$ is infinite, leading to models with excess variables. The lasso isntead minimizes:

\[\text{SSE}+\lambda\sum^p_{j=1}|\beta_j|\]

so the scaled sum is of the absolute values of the betas rather than the squares. This gives the $l1$ norm of the beta vector - the sum of the magnitudes of the vector's components, or the taxicab distance. This causes the lasso to select variables, zeroing the betas for high enough values of $\lambda$. As $\lambda$ rises, predictors are added and dropped unpredictably, and the final model may include any number of them

## Alternate View of the Lasso and Ridge Regression 

These methods may be viewed as linear programming problems, were SSE is minimized subject to the lambda term. In other words, on a p-dimensional system where each beta defines an axis, where does the region demarcating a certain quantity of SSE touch the constraint region? The sum $s$ of squared betas (or absolute values, for the lasso) may be viewed as a budget constraining the estimation of the coefficients. For each value of $\lambda$, $s$ represents a condition on the betas estimated by OLS.  That budget is unique for each value of $\lambda$. The larger the coefficient estimates, the higher this budget. Geometrically, the problem involves finding the point where the contours of the SSE region (ellipses surrounding the point of minimum SSE, increasing in area as SSE increases) touch the constraint region (a diamond for the lasso and a circle for ridge regression). These shapes arise from the differing penalization functions: a linear combination of the absolute values of the betas for the lasso, and linear combination of squares of betas for ridge regression. The shapes of course have more dimensions if there are more predictors.

\[\text{minimize SSE subject to}\sum^p_{j=1}\mid\beta_j\mid <= s\]

In the case of best subset selection, the constraint is:

\[ \sum^p_{j=1}I(\beta_j !=0) <= s \]
i.e., no more than $s$ predictors can be used. The value of $s$ and the SSE minimizes are linked; neither precedes the other, and solving the problem involves finding the point of overlap.

If the area of the constraint function overlaps with minimum of SSE, then a standard OLS estimate solves the problem. If not, the lambda estimate is chosen that corresponds to the smallest SSE that touches the area of the constraint function. This is why the lasso can zero out predictors; the absolute value function that gives the area of the constraint has verities on the axes, so one or more coefficients will equal zero if the SSE region touches that point. 

### Comparing RR and the Lasso

Ridge regression does better if all predictors are related to the response, even marginally, since every true $/beta$ will be nonzero. Where many predictors are irrelevant, the lasso does better. Cross-validation can be used to ascertain which scenario matches the data.

### The Bayesian View of RR and the LAsso

From the Bayesian perspective, the coefficient vector $\beta$ has an unknown prior distribution. Meanwhile, the data has a likelihood distribution \[f(Y\mid{X}, \beta)\], where each observed predictor is its own vector. Multiplying these two gives the posterior distribution:

\[f(Y\mid{X}, \beta)p(\beta)\]
, assuming $X$ is fixed. Assume further that \[p(\beta)=\prod^p_{j=1}g(\beta_j)\], where $g$ is a density function. RR and the lasso correspond to two special cases of this model (making the usual LM assumption of normally distributed residuals):

1. If $g$ is normally distributed with mean zero and $\sigma$ a function of $\lambda$, ridge regression gives the posterior mode (the most likely value) of $\beta$.  

2. If $g$ is a double exponential (Laplace) distribution with mean zero and a scale parameter a function of $\lambda$, then the lasso is the posterior mode of $\beta$, though not the posterior mean. This is a sharply peaked distribution, consistent with many $\beta$s being zero.

This aligns with the form of each method - RR modifies the coefs by a proportion, the lasso by a constant value.


### Illustration: A Simple Case

Say that $n=p$ and X is the identity matrix. Assume also we are ignoring the intercept, so the optimal solution is:

\[\beta_i=y_i\]

Here, the ridge regression estimate is:
\[\beta_j=\frac{y_i}{1+\lambda}\]

while the lasso adds to $y_i$ $\frac{\lambda}{2}$ if $y_i > \frac{\lambda}{2}$, adds it if $y_i < -\frac{\lambda}{2}$, and zeroes betas whose absolute value is less than $\frac{\lambda}{2}$. In other words, ridge regression shrinks coefficients by a proportion, the lasso by a constant. 


### Selecting the Tuning Parameter

This is typically done by cross-validation, selecting whichever $\lambda$ has the smallest error. 


# Dimension Reduction Methods

An alternate solution to many predictors is to transform them into a smaller number of objects. We do this by creating $M <p$ linear combination of the predictors. 

\[Z_m=\sum^p_{j=1}\phi_{jm}X_j\]

with constants $\phi$. Remember each $X$ here is an $n$-length vector; multiplying the $n\times{p}$ data matrix by every eigenvector will create an $n\times{p}$ matrix. Note that there is a separate $\phi$ for each predictor for each $M$ of the transformed variables (e.g., the $p \times m$ loadings matrix in factanal). The model then becomes:

\[y_i = \theta_0 + \sum^M_{m=1}\theta_mz_{im}+\epsilon_i\]

where each $\theta_m$ is the regression weight and each $z_{im}$ is an element of one of the $M$ vectors of transformed values (e.g., a single factor). This is simply OLS using transformed values of the predictors.

It follows that
\[\sum^M_{m=1}\theta_mz_{im} =\sum^M_{j=1}\theta_m\sum^p_{j=1}\phi_{jm}x_ij =\sum^p_{j=1}\sum^M_{j=1}\theta_m\phi_{jm}x_{ij}=\sum^p_{j=1}\beta_jx_{ij}\]

so each predictor's beta is the sum of each component's regression weight multiplied by  the constant $\phi_j$ for that predictor for that component.
\[\beta_j=\sum^M_{m=1}\theta_m\phi_{jm}\]This is thus a special case of linear regression. In all cases, the predictors are transformed, and then the model is fitted.


## Principal Components Overview

Yes, the square root of an eigenvalue represents a component's standard deviation. This is why components with higher eigenvalues capture more variance: the projected data points are spread further along the component.

In PCA, the first principal component (an $m\times{n}$ components matrix formed by multiplying the transpose of $m$ eigenvectors by transposed, scaled data, or alternately the data by the eigenvectors) represents the direction of greatest variation - the vector maximizing distances from the other vectors of the correlation matrix. Projecting observations onto these lines would capture maximum variance. This amounts to translating the greatest shares of variances $X$ into scalar coordinates on the components. It might turn out to be:

\[Z_1 = .839\times(pop - \bar{pop}) + .544\times(ad - \bar{ad})\]

where the coefficients are the loadings $\phi_{11}$ and $\phi_{12}$ of the two predictors - the constants by which the raw scores are multiplied to transform the data. For each component, the loadings are the values of the corresponding eigenvector (reading downwards). Remember in PCA the components are linear combinations of the predictors. Each $z_{in}$ is a principal component score. In factanal, we find these scores and then reconstruct the predictors from them.

Scaling should be done, because PCR is sensitive to units of measure.

The first PC is equivalently the direction of maximum variance (i.e., the line on which the data would be spread farthest apart if projected) or the line that minimizes summed distances from the data to the line - this is what is meant by "capturing" variance. It turns out these two criteria give the same result. These lines are eigenvectors because eigenvectors represent the direction a transforming matrix "points" vectors when used to project them; they satisfy $Av =\lambda{v}$, meaning the eigenvector is _not_ distorted by the matrix, only scaled. Each eigenvector corresponds to the matrix's distorting effect in one dimension. Since they are derived from the correlation (or covaraince) matrix, which translates he data vectors to a a coordinate system that expresses their relationships ($\frac{X^T}{\sigma_{X}}\frac{X}{\sigma_{X}}\times \frac{1}{n}$), eigenvectors represent directions of maximum variance. Projecting points onto the PC - achieved by multiplying by the loadings - thus maximizes their spread along the PC line. Subsequent components must be uncorrelated (orthogonal; i.e., dot product of zero) in order to capture remaining variance. The regression is fitted using the transformed components as x-values.

This plot shows how eigenvectors retain their position after a matrix transformation. Since the correlation is inverse, the main axis is negative. _Eigenvectors for a symmetric real matrix are orthogonal (dot product 0)_.

The eigenvectors of this two-dimensional matrix have elements of absolute value about .7017, which corresponds to 45 degrees on the unit circle.They are orthogonal. 

```{r}
dat <- mtcars %>% select(wt, mpg) %>% 
  cor()
eigens <- dat %>% eigen()
  
dat <- dat %>% as.matrix() %>%
  as.data.frame() %>%
  # rownames_to_column() %>% 
  # pivot_longer(c(wt, mpg)) %>% 
  # unite(rowname, name, col = "name") %>% 
  cbind(t(eigens$vectors))

ggplot(data = dat, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_segment(aes(xend = 0, yend = 0))+
  geom_point(aes(x = `1`, y = `2`))+
  geom_segment(aes(x =`1`, y = `2`, xend = 0, yend =0), col = "blue") +
  xlim(c(-1.5,1.5)) +
  ylim(c(-1.5,1.5)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  labs(title = "Eigens and Matrix Vectors")

eigens <- as.matrix(dat[,1:2]) %*% eigens$vectors
dimnames(eigens) <- NULL#{map2_dfr(as.data.frame(.$vectors), .$values, `*`)}

cbind(dat[,1:2], eigens %>% t) %>% 
  ggplot(aes(x = wt, y = mpg)) +
  geom_point() +
  geom_segment(aes(xend = 0, yend = 0))+
  scale_color_manual(values = c( x ="red", y ="green")) +
  geom_point(aes(x = `1`, y = `2`))+
  geom_segment(aes(x =`1`, y = `2`, xend = 0, yend =0), col = "blue") +
  xlim(c(-1.5,1.5)) +
  ylim(c(-1.5,1.5)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_segment(aes(xend =0, yend = 0, x = 0, y = 1), col = "green") +
  geom_segment(aes(xend =0, yend = 0, x = 1, y = 0), col = "red") +
  labs(title = "Eigenvectors after Matrix Transformation")
```


The eigenvectors of the covariance or correlation matrix give these directions of maximal variance. (Note $\frac{X^TX}{n-1}$ gives the covariance matrix). The correlation/covaraince matrix itself really expresses the data with an alternate basis giving relative relationships, compressing or elongating the basis vectors in the directions of most variance. If data are plotted after transformation by the cor/cov matrix, they will cluster along the eignvectors - the axes of the transformation, where variance is maximal. All are orthogonal to each other, maximizing variance captured when projecting data onto the eigenvectors. Constructing components involves taking the desired number of eigenvectors and multiplying the scaled data by the eigenvectors matrix; each component is the matrix-vector product of the data matrix and an eigenvector. A single element of component vector $j$ is \[y = zv^T\], where $v$ is an element of a column of the eigens matrix.  Additional components can be found by subtracting the first component from $X$ and then finding the weight vector that maximizes captured variance. It turns out this is always the eigenvector with the next-highest eigenvalue. From another standpoint, each additional component explains as much as possible of the error produced by the previous one. The equation to project the data onto the components is:

\[Y = V^TZ^T\] or \[Y = (ZT)^T\], where $V$ is a $p\times p$ (or$p\times{m}$, if $m$ eigenvectors are chosen) eigenvector matrix that transforms each standardized $z_{ij}$ of Z into a reproduction of the response. This is equivalent to translating the data to a coordinate system where each component forms the axis of  a dimension. This works for any number of components, since $Z$ has as many columns as each eigenvector has rows.  Maximizing variance means maximizing $X^TX$ (i.e., squared distances). This is a positive semidefinite matrix($z^tMz$, the dot product of a vector and the matrix-vector product of that vector and the matrix, is positive or zero for all real $z$). The first eigenvector gives the maximum quantity for such a matrix.

To project the whole data  on tho the components (a $n\timesp$ matrix), compute $Z*=ZV$, using the full eigens matrix. This gives the transpose of the $Y$ defined above

Illustration:

```{r}
tibble(PC1=-20:20, PC2=rnorm(mean = 0, sd = 5, n = 41)) %>% 
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point()+
  geom_hline(yintercept = 0) +
  scale_y_continuous(limits = c(-10, 10)) +
  geom_segment(aes(x = PC1, xend = PC1, y = 0, yend = PC2), linetype = "dashed") +
  geom_point(aes(y = 0), shape = 4)+
  geom_vline(aes(xintercept = 0))


```


For the example given above, each value of the first PC can be seen as a one-number summary of the scores of pop and ad for that observation. A negative score indicates below-average values for each (since each loading is positive). The first PC is positively correlated with each predictor. Since the second PC must be orthogonal (simply perpendicular in 2D space), the loadings are .0544 and -.0839 (the sign of ad's  second loading reversed). This component has lower scores, indicating it captures less information. It has weaker correlation with either predictor. It is easy to see from the plot that a second component perpendicular to the first completely describes the position of each point, since there are just two predictors.

Geometrically, once the data are projected onto the first component, each subsequent component is fitted on the residuals (the variance unexplained by the projection), which is why they are orthogonal; orthogonal vectors have dot product 0, which is the same as zero correlation. Together, the components form an alternate coordinate system.

The main benefit of all this is it saves us from estimating \[\frac{p(p-1)}{p}\] parameters for a full model, largely the covariances.

### Principal Components Regression

PCR assumes that the directions of most variation in $X$ - the components - are those associated with $Y$. By capturing only relevant variacne, we can get a better model than a standard OLS fit. Adding components reduces bias but increases variance (of course), giving the typical U curve for MSE. PCR does well comapred to the lasso and RR if only a few PC are necessary and poorly if many are required. Components are also not easily interpretable.

PCR is not truly a feature selection method, since each PC is a linear combination of every predictor. The number of components is usually chosen by cross-validation. As with RR, standardizing is crucial to prevent some predictors from having too much impact.


### Excess Detail: Why Is It Eigenvectors?

https://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit

Any square matrix can be decomposed as $A=P\Lambda{P^{-1}}$, where $P$ is the eigenvectors matrix and $D$ is the diagonal matrix of the eigenvalues (each eigen on the trace, zeroes otherwise). The amth is simple (where $Q$ is the full eigens matrix)
\[Av = \lambda{v}\\
AQ=Q\Lambda\\
A=Q\Lambda{Q^{-1}}\]

This works because $Av=\lambda{v}$. Scaling the eigenvector inverse by the eigenvalues, _scalars which for the eigenvectors represent the effect of the matrix transformation_, yields a matrix which yields the original matrix when multiplied by the eigenvectors, instead of just the identity (the usual product of matrix and inverse).


```{r}
example <- cor(mtcars)
example
eigen(example)$vectors %*% diag(eigen(example)$values) %*% solve(eigen(example)$vectors)
```


PCA seeks to project data onto whichever direction has the highest variance of the new coordinates. If $C$ is the covaraince matrix, we want a unit-length vector $w$ ($w^Tw=1$) such that $w^TCw$ is as high as possible (since the sum of each column of $C$ is its share of total variance). $w$ must be unit-length ($||w||=1$) because making it longer would allow us to make the expression infinite. This projection would be $Xw$ with variance $\frac{n}{n-1}(Xw)^T\cdot{Xw}$.

The usual explantation involves adding a Lagrange multiplier and differentiating, which gives the eigenvector equation. Substituting into the maximization function gives $\lambda$; since we are maximizing, we want the eigenvector with the highest eigenvalue.

Eigendecomposition is easier to follow. With he eigendecomposition $Q=C\Lambda{C^{-1}}$, $w^TCw$ can be rewritten $\sum{w^2_i}\lambda_i$ (each element of $w$ squared and scaled by the eigenvalue). Here the $w_i$ are eigenvectors.  Clearly, this is maximized if $w = (1,0,0,\dots,0)$ - that is, the first eigenvector, giving variance $\lambda_1$. 

This follows from the spectral theorem. Assume the first eigenvector of the symmetric matrix $C$, $w_1$ replaces the first basis vector (such that $w_1 = (1,0,0...0)$. In other words, we substitute the first eigenvector for $\hat{i}$.  Any matrix is really a linear combination of basis vectors scaled by varying quantities.  PCR software outputs the values of the eigenvectors in the _original_ coordinate system.  The first element of $C$'s trace becomes $\lambda_1$, because $Cw_1=\lambda_1w_1 = (\lambda_1,0,0,0...0)$ - i.e., by definition, multiplying the first column of  $C$ by the eigenvector is the same as scaling the eigenvector by its eigenvalue. In other words, the matrix is decomposed as a linear combination of basis eigenvectors and eigenvalue scalars. The matrix is symmetric. So C becomes:
\[C =
\begin{pmatrix}
\lambda_1&0&\dots&0\\
0\\
.\\
.\\
.\\
0
\end{pmatrix}\]
(remember this is just the identity multiplied by the diagonal eigenvalue matrix).

Equivalently, each eigenvector can be seen as a 1D subspace in which $C$ behaves like the scalar $\lambda_i$ 

The whole matrix can be filled in by repeating the same algorithm (eigenvector replacing each basis vector, with $\lambda_i$ on the trace) for each block, since it is symmetric.

Given the unit-length constraint, $w^TCw$ is obviously maximized with basis vectors for $w$ - $w= (1,0,0...0)$, etc. In the eigenbasis, these equate to the diagonal matrix of eigenvalues. Therefore, each eigenvalue gives the maximum variance that can be extracted by a single $w$,a and the corresponding $w_p$ are the eigenvectors of $C$.

### Partial Least Squares

PCR is unsupervised - the response $Y$ plays no role in selecting the components. Partial least squares differs from PCR in this respect. After standardization, each loading $\phi_j1$ is obtained from $\beta_j$ of a regression of $Y$ onto $X_j$. So each loading represents the average change of the response in response to that predictor alone, no eigens involved.  These coefficients are proportional to correlations, so the predictors most closely related to the response receive higher loadings. To find additional directions, we regress each variable on $Z_1$ (the first component) and take the residuals. These represent variation unexplained by the first component. We then regress each predictor on each set of residuals and obtain $Z_2$ the same way as $Z_1$. Once all loadings are obtained, the model is fit in the same fashion as PCR.

Overall, supervision increases variance relative to PCR but reduces bias.

# COnsiderations for Higher Dimensions

Traditional regression techniques struggle for high-dimensional settings, increasingly common with enhanced data collection, with hundreds of measurements for each observation. When $p >n$, OLS gives a perfect fit of no use in predicting test data. There is no unique solution because the predictors matrix spans the dimension of the response vector; the response can be perfectly reproduced by many linear combinations of $X$. Intuitively, a straight line can connect all the points with no residual. In other words, unmodified OLS is too flexible. This holds true even if the predictors are totally unrelated to the response. However, adding more predictors hugely increases test MSE by increasing variance.

$C_p$, the BIC, and the AIC are of little help because $\hat\sigma^2$ cannot be reliably estimated with very many predictors. The methods discussed above address these problems by reducing the flexibility of linear modeling by shrinking or eliminating parameters. The lasso, for instance, has degrees of freedom equal to the number of nonzero coefs estimated.

This is the curse of dimensionality: additional features improve the fit only if truly associated with the response.


In this stupid model, there are no degrees of freedom, so no residuals whatsoever.


```{r}
dat <- replicate(100, sample(-(10^4):10^4, 10, replace = T)) %>% as.matrix() %>%
  cbind(., y = rnorm(10) + 2) %>% 
  as.data.frame()
dat %>% lm(data =., y ~.) %>% summary
```

### Interpreting Results in High Dimensions

High-dimensional settings are highly multicollinear - any predictor can be expressed as linear combination of the others. It follows that we cannot isolate any predictors as truly influential, nor find optimal regression coefficients. Each good-fitting model is only one of many possibilities. Additionally, training set measurements like $R^2$ 2are useless for predicting performance on high-dimensional data.




