---
title: "ISLR Chapter 6 Notes"
author: "Ryan Heslin"
date: "12/27/20"
output: html_document
---

# The Basics of Decision Trees

Tree-based methods split the predictor space into regions, usually by local means or modes. These splits can be mapped to tree diagrams. Simple tree methods are not very effective, but modern improvements like random forests and boosting that average many tree predictions are competitive.


# Review Questions


1. How are regression trees fitted?
2. Why do individual trees usually perform badly?
3. How can pruning result in better trees? How is $\alpha$ used to select them?
4. What is the standard pruning algorithm?
5. Why are the Gini index and entropy better objective functions than classification error?
6. What is the basic idea behind bagging?
7. What flaw of bagging does a random forest correct?
8. What algorithm does boosting use, and what are its parameters?

# A Simple Example Tree

Consider a model that predicts a baseball player's salary. For each region, the prediction is the log salary mean. Players with less than 4.5 years are grouped into one region; those with more experience are subdivided into those with more or fewer than 117.5 hits.(These are cutoffs, not actual data points). 


```{r}
library(tidyverse)
library(ISLR)

hitters <- Hitters

hitters %>% na.omit() %>% 
  mutate(region = case_when(Years < 4.5 ~ 1, Hits < 117.5 ~2, TRUE ~ 3) %>% as_factor()) %>% 
  ggplot(aes(x = Years, y = Hits, col = region)) +
    geom_point()+
    geom_vline(xintercept = 4.5) +
    geom_hline(yintercept = 117.5)
```

The final regions are called _leaves_ or terminal nodes. Decision points above them are internal nodes. This example tree assumes hitting prowess only matters for players who have several years' experience. 

These models are simple and easy to understand.

## Prediction by Stratification

The tree is built as follows:

1. Divide predictor space into $J$ distinct regions.
2. For every observation in region $R_j$, make the same prediction. 

The regions are usually formed by dividing the predictor space into hyperdimensional "boxes" that minimize combined SSE:
\[\sum_{j=1}^J\sum_{i\in{R_J}}(y_i-\hat{y_{R_j}})^2\]

whether $\hat{y_R_j}$is the mean of the response for each box's training observations. For computational reasons, the boxes are usually fitted by  recursive binary splitting. This greedy approach selects the best split at each step, ignoring the effects of future steps. Like forward subset selection, it ignores initially worse additions to the model that might lead to optimal results later on.

The initial split is found by selecting a cutpoint $s$ of predictor $X_j$ such that $X_J <s$ and $X | X_J >=s$ minimizes RSS as much as possible for each region together. The exact loss function for the first split is:

\[\sum_{i: x_i\in{R_1(j,s)}}(y_i-\hat{y}_{R_1})^2+
\sum_{i: x_i\in{R_2(j,s)}}(y_i-\hat{y}_{R_2})^2\]

The two $\hat{y}$ are mean responses for the training observations in each region. Note we have to consider all possible cutpoints for all predictors. Once the initial split is made, the process is repeated for each region, but only _one_ is split this time. Each iteration $n$ make one split of the $n$ regions generated by the preceding operations until some stopping point is reached. 

## Pruning

Overcomplicated trees lead to overfitting. Using some SSE improvement threshold as a stopping criterion would not help because splits that do not improve RSS at first may lead to later splits that greatly reduce it. 

A better way is to generate a very large tree, $T_0$, and then pick a subtree with fewer leaves likely to have low test error. But cross-validating many trees is unfeasible. The solution is _cost complexity/weakest link pruning_. We select a nonengative tuning parameter $\alpha$. For every value of $\alpha$, some subtree will minimize:

\[\sum_{m=1}^{|T|}\sum_{i:x_i\in{R_M}}(y_i-\hat{y}_{R_m})^2 +\alpha|T|\]

where $|T|$ is the number of terminal nodes, $R_m$ the rectangle of predictor space corresponding to each terminal node (each with a local mean used as the prediction), and $\hat{y}_{R_m}$ the predicted response for each terminal node (the mean of observations in $R_m$). $\alpha$ penalizes trees for complexity; each extra terminal node adds $\alpha$ to the sum to minimize. When it is 0, the entire $T_0$ is selected, but as it increases, simpler trees become optimal. The least useful nodes are pruned in sequence. THe optimal level of $\alpha$ is selected by cross validation, then the corresponding subtree is chosen. 

Algorithm:

1. Use recursive binary splitting to grow large tree on training data, stopping at some number of observations threshold. 

2. Prune large tree using $\alpha$ to obtain sequence of best subtrees as a function of alpha, pruning the node that least minimizes SSE at each step.

3. Use k-folding to select $\alpha$. For each fold:

a. Repeat 1 and 2 on all but the holdout fold.

b. Evaluate MSE on data in holdout fold as a function of $\alpha$ (i.e., $\alpha\times{T}$)

c. Compute and average results for each chosen value of $\alpha$ and pick one that yields the subtree that minimizes MSE.

4. Choose subtree that corresponds to chosen value of $\alpha$.

As ever, trees with more leaves are more flexible and therefore more variant but less biased. Adding more leaves, like adding more predictors, always improves train error but begins to increase test error past some point.

## Classification Trees

Trees are adapted to classification by predicting responses by the most common class of each terminal node instead of the mean of local observations. Classification error rate instead of RSS can be used for binary splitting; since we classify by most common class, it is simply the fraction of observations in each region that belong to _any other class_:

\[E = 1 - max_k(\hat{p}_{mk})\]

where $\hat{p}_{mk}$ is the proportion of training observations in region $m$ from class $k$.

However, this approach on its own is not sensitive enough, since it ignores the uneven prior probabilities of classes. A naiive classifier can perform well if one class predominates. These two approaches lead to purer terminal nodes and therefore lower test error.

1. The Gini index 

\[G = \sum^K_{k=1}\hat{p}_{mk}(1-\hat{p}_{mk})\]
or the summed product of each class's proportion in the region with that of its complement (the proportion belonging to every other class). This equates to the variance of the distribution of the class probabilities (sum of squared differences of class probabilities). This is also the variance of the binomial distribution This represents the classification error probability of assigning an item. The higher the index, the more variation among classes in the region. So a region with just one class would have a Gini index of 0 ($1(1-1))$, indicating no possibility of classification error, while a node with an even split of two classes would have an index of $2(0.5(1-0.5)) = 0.5$, indicating a 50% error rate.

2. The entropy function:

\[D = -\sum_{k=1}^K\hat{p}_{mk}\log\hat{p}_{mk}\]
or the summed product of each probability and that same probability in log units (converted to positive, since logs of proportions are negative).
Since logs of proportions are higher in absolute value the lower the proportion(e.g., $\log(.01) > \log(.1)$), the value is higher the more diverse the classes in the node; $P_mk$ of 0 and 1 both correspond to 0. A variant of this function is used for deviance.

These approaches are better indices of node purity than simple classification error.

Note also that classes may have to be lumped to enable binary splitting. An education predictor with several levels might be split into high school and college or more categories if more than two are present in a region. Splits may yield terminal nodes that predict the same value. This promotes _node purity_: the degree to which a class of the _response_ dominates a node's predictions. This way, we can be more confident test observations assigned to that node have its response value, since it proved more accurate on the training data. This kind of splitting does not improves classification error at all but optimizes the Gini ratio and entropy, which both measure node purity.

## Trees vs. Regression

Trees predict by the mean of observations at each terminal node, with the form:

\[f(X) = \sum_{m=1}^Mc_m\cdot1_{X\in{R_m)}}\]

where each $M$ is a split of predictor space and $c_m$ is the regional mean. Trees perform best with very nonlinear relationships where linear regression would struggle. In classification problems, this means nonlinear decision boundaries. As ever, test error is the best means of comparison. Trees are also easier to interpret and graphically display,a and can handle categorical predictors without the awkwardness of dummy variables.

Individual trees are not very robust!

# Bagging, Random Forests, Boosting

These three techniques improve the basic tree approach to create more sophisticated models.

## Bagging

Trees are highly variant; very different trees will emerge from different initial partitions of training data. Bagging (bootstrap aggregation) can correct this. _Each_ of a set of independent observations has variance $\sigma^2$ (the average observation's squared distance from the mean of $X$, given infinite samples, is the same as $X$'s variance). But the variance of the mean $Z$ of the _set_ of observations is $\sigma^2/n$ - by averaging, we can get more reliable estimates (infinitely reliable, with infinite samples). You've just rediscovered the standard error, genius.

In practice, this means bootstrapping many training sets, developing models from each independently, and averaging the predictors. Given $B$ training sets:

\[\hat{F}_{avg}(X)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{(b)}x)\]

To simulate multiple training sets, we just bootstrap the single set available.

If we use bagging on regression trees, we can also avoid pruning, because averaging many trees corrects the variance of individual trees. For classification, we assign to whichever class was predicted most (majority vote) or average each tree's confidence in its class prediction for that observation.

$B$, obviously, should be set before the returns of additional averages diminish. 

Each bagged tree uses about 66% of the training observations. (This follows from the limit of the probability of the $ith$ sampled observation appearing in the bootstrap, since they are sampled with replacement). The leftover third are called out-of-bag (OOB) observation. FOr each observation $i$, we can predict it using the $B/3$ trees that were not fitted on that observation. We can compute MSE or classification error by averaging OOB error for each observation. For enough bags, OOB error approaches LOOCV error.

Unfortunately, predictions obtained from bagging are hard to interpret and obscure the importance of variables. We can approximate variable importance by recording the average MSE reduction for splits of each predictor (all splits across all trees) (or Gini index reduction, if we are classifying).

## Random Forests

Random forests improve on bagged trees with a small change. For each split in each bagged tree, the predictor $p$ to split on is chosen from a random subset $m\in{p}$, not all predictors. Resampling is done for each split, with $m\approx{\sqrt{p}}$. In the standard bagging approach, most trees will choose the best predictor for the initial split (a few won't due to randomness). This means the predictions are correlated, limiting the varaince reduction from averaging. With random forests, many splits will ignore the strong predictor, increasing variaince among the trees. Some trees will therefore expose relationships among weaker predictors a bagged forest would have missed.

$m$ should generally be small if there are many highly correlated predictors (to increase diversity of predictors chosen for splits).

## Boosting

Boosting differs from bagging in that it grows trees sequentially. Analogous to backfitting, it fits each tree on the residuals of the previous tree. Then the shrunken new tree is added to the previous trees to update the model. (If new trees were not shrunken, the predictions - the sums of predictions of the trees at each iteration of the model - would be far too large). Each individual tree is typically small; it may be a "stump" with just two nodes. The paramter $\lambda$ further reduces the predictions of each new tree. The process slowly refines the overall estimate $\hat{f}(x)$ and reduces the residuals, with each step improving an area of the current model that does badly. 

1. Take the data $y_i$ as residuals $r_i$ and set $\hat{f}(x)=0$.

2. Up to $B$;
a. Fit a tree $\hat{f^{(b)}}$ with $d$ splits and $d+1$ terminal nodes on the residuals.

b. Update $\hat{f}$ with a shrunken version of the new tree:
\[\hat{f}(x)=\hat{f}(x)+\lambda{\hat{f^{(b)}}}(x)\]

c. Update the residuals by subtracting the new estimate:

\[r_i - \lambda{\hat{f^{(b)}}}(x_i)\]

3. Output boosted model:

\[\hat{f}(x)=\sum_{b=1}^B\lambda{\hat{f^{(b)}}}(x)\]

The tuning paramters are:

1. Number of trees, $B$. THis can cause overfitting if too large, unlike with bagging. Select by cross-validation.

2. Shrinkage parameter $\lambda$, a small positive number, often .01 or .001. Smaller values require a larger $B$ for good performance.

3. $d$, number of splits per tree. $d=1$ results in stumps. It controls the interactions of the full (ensemble) model, since $d$ splits involve at most $d$ predictors.


