




# Introduction
We use the svm function of the e1071 library. The relevant args are kernel and cost (for tolerating margin errors; higher values narrow margins).

An annoying blue outlier prevents a linear separation
```{r}

library(tidyverse)
library(rlang)
set.seed(1)
X <-  matrix(rnorm(20*2), ncol = 2)
Y <-  c(rep(-1, 10), rep(1,10))
X[Y==1,] <- X[Y==1,] +1
X %>% 
  data.frame() %>% 
  mutate(y = Y) %>% 
  ggplot(aes(X1, X2, col = factor(y))) +
    geom_point()
```

Now use SVM. Factor coercion is necessary if we don't want SVM regression
```{r}
library(e1071)
dat <- tibble(X1 = X[,1], X2 = X[,2], Y = factor(Y))
svm1 <- svm(Y~.,data = dat, kernel = "linear", cost =10, scale = FALSE)
svm1
```

Using base plotting, the support vectors are circles and the other observations crosses. Index shows which are which.
```{r}
plot(svm1, dat)
dat[svm1$index,]
summary(svm1)
```

If we try a smaller cost, we get a much bigger margin and many more support vectors. We can't get the margin or the hyperplane equation directly, unfortunately
```{r}
svm2 <- svm(Y~.,data = dat, kernel = "linear", cost =.1, scale = FALSE)
svm2
plot(svm2, dat)

```

We can use tune to cross-validate. The best model has 16 support vectors and cost of .1, indicating a wide margin.
```{r}
set.seed(1)
tuned <- tune(svm, Y~., data = dat, kernel="linear",
              ranges = list(cost=c(.001,.01,.1,1,5,10,100)))
summary(tuned)$performances %>%  arrange(error)
summary(tuned$best.model)
best <- tuned$best.model
```

The predict method works as you'd expect:
```{r}
set.seed(1)
X_test <- matrix(rnorm(20*2), ncol = 2)
Y_test <- sample(c(-1,1), 20, replace = TRUE)
X_test[Y_test==1] <- X_test[Y_test ==1] +1
test_dat <- tibble(X1 = X[,1], X2 = X[,2], Y = as.factor(Y_test))
```

```{r}
ypred <- predict(best, test_dat)
table(ypred, truth = test_dat$Y)
```

Should a separable hyperplane exist, svm() will find it. We ensure it does (now some X'es are shifted by the SD of both predictors)

```{r}
X[Y==1,] <- X[Y==1,]+0.5
plot(X, col=2, pch=19)
```

Set cost much higher to ensure very thin margins. We see there are only three support vectors, and no training errors are made. This is probably overfit.
```{r}
dat <- tibble(X1 = X[,1], X2 = X[,2], Y = as.factor(Y))
true_svm <- svm(Y~., data = dat, kernel = "linear", cost =1e5, scale = FALSE)
plot(true_svm, dat)
```

Obviously, just change the arg of kernel. The relevant params are degree and gamma

## The Support Vector Machine
```{r}
set.seed(1)
X <- matrix(rnorm(200*2), ncol =2)
X[1:100,] <- X[1:100,]+2
X[101:150,] <- X[101:150,] -2
Y <- c(rep(1, 150), rep(2, 50))
dat <- tibble(X1 = X[,1], X2 = X[,2], Y = as.factor(Y))
svm_call <- partial(svm, formula = Y~.,scale = FALSE, ... = )
plot(X, col = Y)
```

Now split up data and try a radial kernel. Too many training errors, so we should cross-validate $\gamma$.
```{r}
train <- sample(200, 100)

radial <- svm_call(kernel = "radial", gamma = 1, data = dat[train,], cost = 1)
summary(radial)
```

It turns out a cost of 1 and $\gamma$ of 2 optimize the fit:
```{r}
radial_tuned <- tune(svm, Y~., data = dat[train,], kernel = "radial", ranges = list(cost = c(.1,1,20,100,1000), gamma = c(.5,1,2,3,4)), decision.values=TRUE)
summary(radial_tuned)$performance %>% arrange(error)
rad <- summary(radial_tuned)$best.model
```

## ROC Curves

We use the ROCR package to produce ROC curves. (Recall an ROC curve plots true positive rate against false positive rate - something like an odds ratio at different classification threshold. A perfect model has an area under the curve of 1).

We can get predicted values from the model object. They all have low magnitude.

```{r}
library(ROCR)
plot_roc <- function(pred, truth, ...){
  predob <- prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf)
}
```

```{r}
rad <- svm_call(kernel = "radial", data = dat[train,], gamma =2, cost =1, decision.values = TRUE)
fitted <- rad$decision.values

plot_roc(fitted, dat[train, "Y"])
```
These plots are good for comparing different levels of $\gamma$.

Non-binary classification is done automatically using the one-vs-one approach.

## Application: Gene Expression

This is contained in Khan, a dataset of tissue samples from four types of tumor. It has training and test sets.
```{r}
khan <- ISLR::Khan

map(list(khan$ytrain, khan$ytest), table)
```

This is a typical $p>n$ dataset where a hyperplane is useful. We can just a linear kernel, and the model does perfectly! But that's to be expected with this structure.

```{r}
khan_train <- tibble(X = khan$xtrain, Y = as.factor(khan$ytrain))
mod_khan <- svm_call(data = khan_train, kernel = "linear", cost = 10)
summary(mod_khan)

table(mod_khan$fitted, khan_train$Y)
```

But the model makes few errors on the test set:
```{r}
khan_test <- tibble(X = khan$xtest, Y =as.factor(khan$ytest))

khan_pred <- predict(mod_khan, khan_test)

table(khan_pred, khan_test$Y)

pred_accuracy <- function(tab){
  sum(diag(tab))/sum(tab)
}
```


# Conceptual

## 1.

The first problem asks us to sketch some 2D hyperplanes
```{r}
planes <- tibble(plane = c("1", "2"), slope = c(3, -.5), b_0 = c(-1, 2))

expand_grid(x = seq(-5, 5, by = .2), y= seq(-5, 5, by = .2)) %>% 
  mutate(p1 = 1 + 3*x -y >0, p2 = -2 + x +2*y >0) %>% 
  ggplot(aes(x = x, y = y, col = p1, stroke = p2))+
  geom_point(size = 2, alpha = .5)+
  geom_abline(data = planes, aes(slope = slope, intercept = b_0, color = plane))
```

## 2.
We are asked to do the same for a polynomial hyperplane. It turns out to be an ellipse:

```{r}
expand_grid(x = seq(-5, 5, by = .2), y= seq(-5, 5, by = .2)) %>% 
  mutate(Class = factor(x^2 +2*x + y^2 -4*y + 1 > 0, labels = c("red", "blue"))) %>% 
  ggplot(aes(x = x, y = y, col = Class)) +
  geom_point(size = 2, alpha = .5)
```

And then to classify several points:

```{r}
hyper <- expr((1 +x)^2 + (2-y)^2 <=4)

expand_grid(x = -5:5, y = -5:5) %>% list2env()

eval(hyper, env = expand_grid(x = -5:5, y = -5:5)) %>% 
  factor(labels = c("blue", "red"))
```

We are asked to consider polynomial hyperplanes.

It is trivial to show the polynomial is a nonlinear transformation.

\[T(x,y)= \begin{bmatrix}x^2+2x\\y^2-4y\end{bmatrix}\]
\[T(c(x,y)) = \begin{bmatrix}c^2x^2+2cx\\c^2y^2-4cy\end{bmatrix}\]
\[cT(x,y) = \begin{bmatrix}cx^2+2cx\\cy^2-4cy\end{bmatrix}\]
Bzzt. Nonlinear.

If instead we treat the the squared terms as vectors, the transformation is linear:

\[T_2(x^2, y^2, x, y)=\begin{bmatrix}
x^2\\
2x\\
y^2\\
-4y\end{bmatrix}\]

\[cT_2 =\begin{bmatrix}
cx^2\\
c2x\\
cy^2\\
-4cy\end{bmatrix}\]

\[T_2c(x^2, y^2, x, y) =\begin{bmatrix}
cx^2\\
c2x\\
cy^2\\
-4cy\end{bmatrix}\]

I'm too lazy to prove additive associativity as well but it's pretty obvious.

## 3.

We are now asked to consider the MMC for the following data. We see by plotting it cleanly separates observations 2 and 5 and 3 and 6:

```{r}
dat2 <- tibble(obs = 1:7, x_1 = c(3,2,4,1,2,4,4),
         x_2 = c(4,2,4,4,1,3,1),
         class = as.factor(c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue")))

dat2 %>% ggplot(aes(x = x_1, y = x_2, col = class)) +
  geom_point() +
  geom_text(aes(label = obs), nudge_y = -.25, col = "black" ) +
  scale_color_manual(values = c("blue", "red")) +
  geom_abline(slope = 1, intercept = -0.5)

```

I inexplicably attempt linear regression on a categorical response variable
```{r}
a_trans_a <- t(dat2[,-c(1,4)]) %*% as.matrix(dat2[-c(1,4)])
b <- ifelse(dat2$class == "Red", -1, 1)
as.matrix(dat2[,2:3]) %*% solve(a_trans_a) %*% t(dat2[,-c(1,4)]) %*% b

```

I crudely figure out the coefficients, then plot the line.
```{r}
b_1 <- (dat2[3, "x_2"] - dat2[2, "x_2"]) /(dat2[3, "x_1"] -dat2[2, "x_1"])
b_2 <- -b_1
b_0 <- -(dat2[2, "x_2"] - dat2[5, "x_2"])/2
```

The equation comes to :
\[0.5 -X_1 +X_2=0\]
We see it correctly classifies. There are four support vectors: observations 2,5,3, and 6. Since observation 7 is not one of them, moving it would not impact the margin.
```{r}
eqn <- expr(0.5 - x_1 +x_2)
fits <- eval(eqn, env = dat2)
cols <- ifelse(fits >0, "Red", "Blue")
```

# Applied
## 4.

Here we are asked to demonstrate that nonlinear kernels are superior when the decision boundary is nonlinear.

The polynomial model actually does worse. It misclassifed almost every -1 as 1. I squared the values of x1 for the 1 class, which must be the cause.

A standard polynomial model is no good, since $x1$ is really a piecewise function. Maybe a radial one would work?
```{r}
dat <- tibble(x1 = rnorm(100, sd = 2), x2 =  rnorm(100, sd =1), Y = as.factor(c(rep(-1, 50), rep(1, 50)))) %>% 
  mutate(x1 = if_else(Y==1, x1^2, x1))
inds <- sample(100, 50, replace = FALSE)
train <- dat[inds,]
test <- dat[-inds,]

linear <- svm_call(kernel="linear", data = train)
poly <- svm_call(kernel = "polynomial", degree = 2, data = train)

summary(linear)
summary(poly)

lin_pred <- predict(linear, test)
poly_pred <- predict(poly, test)

table(lin_pred, test$Y)
table(poly_pred, test$Y)
```

A radial margin with big margins and low gamma does okay. None of these models can really handle the silly piecewise polynomial I created, though.
```{r}

tune(svm, Y~., kernel = "radial", ranges = list(cost = c(.001, .01,.1,1,2,5,10,25), gamma = c(.001, .005,.01,1,2,3,5,10,25,50)), data = dat)
rad <- svm_call(kernel = "radial", cost = 2, gamma = .01, data = train)
rad_pred <- predict(rad, test)
table(rad_pred, test$Y)

```

## 5.
We are now asked to develop a nonlinear decision boundary through logistic regression.

The plot shows we clearly need two hyperplanes, since the class is determined by a difference of squares, resulting in decision boundaries with slopes $y = \pm{x}$

```{r}
dat <- tibble(x1 = runif(500) - 0.5, x2 = runif(500) -0.5, Y = as.factor(ifelse(x1^2-x2^2 > 0, 1, -1))) 

dat %>% ggplot(aes(x = x1, y = x2, col = Y))+
  geom_point() +
  geom_abline(slope = c(-1, 1), intercept = 0) 

```

The decision boundary from a logistic model is obviously linear, and obviously wrong.
```{r}
inds <- sample(500,250, replace = FALSE) 
train <- dat[inds,]
test <- dat[-inds,]
log_mod <- glm(Y ~., data = dat, family = "binomial")
summary(log_mod)
train_pred <- predict(log_mod, train) %>% 
  {if_else(. <=0, -1, 1)}
train %>% mutate(train_pred = train_pred) %>% 
ggplot(aes(x = x1, y = x2, col = factor(train_pred))) +
  geom_point()
```

I cheat shamelessly by using an absolute value transformation, approximating the function applied to the data. The decision boundary is plainly nonlinear.
```{r}
log_poly <- glm(dat = train, Y~abs(x1) + abs(x2), family = "binomial")
summary(log_poly)

train_pred2 <- predict(log_poly, train) %>% 
  {if_else(. <=0, -1, 1)}
train %>% mutate(train_pred = train_pred2) %>% 
ggplot(aes(x = x1, y = x2, col = factor(train_pred))) +
  geom_point()
```

Now for an SVM. A simple polynomial model fails badly, classifying nearly every point as 1. Since both predictors are uniformly distributed with $\mu=0$, this makes sense; a linear combination of squares would never be negative.

The linear model does no better.
```{r}
svm_mod <- svm_call(data = train, kernel = "polynomial", degree = 2)

svm_linear <- svm_call(data = train, kernel = "linear")

svm_pred <- predict(svm_mod, test)
table(svm_pred, test$Y)

test %>% mutate(svm_pred = svm_pred) %>% 
  ggplot(aes(x = x1, y = x2, col = svm_pred)) +
  geom_point()

train %>% mutate(fitted = svm_mod$fitted) %>% 
  ggplot(aes(x = x1, y = x2, col = svm_pred)) +
  geom_point()

train %>% mutate(fitted = svm_linear$fitted) %>% 
  ggplot(aes(x = x1, y = x2, col = svm_pred)) +
  geom_point()

```

I'm not sure anything can beat the absolute value model, since the true class depends on which predictor has a greater absolute value.

##7. 
This last problem sets us to work on Auto.

It seems models with cost close to default did best, suggesting wide margins were favored.
```{r}
auto <- ISLR::Auto
auto$Y <- with(auto, ifelse(mpg > median(mpg), 1, 0))


costs <- expand.grid(1:9, c(10^(-4:1), 5, 10)) %>% 
  reduce(`*`) 

auto_svm <- tune(svm, data = auto, ranges = list(cost = costs), Y~., kernel = "linear")

auto_svm$performances %>% arrange(error)
```

Oddly, radial models have the highest error, declining as cost falls before increasing. Polynomial error is consistent across costs. Linear error varies inversely with cost. I supplied some very low cost values that led to excessive margins, I think. 

Gamma has barely any effect on error rate. In isolation, it seems other parameters have little effect on error rate.

I finish off by plotting every pair of predictors, because I can. The decision boundaries are clearest with horsepower and weight.
```{r}
gammas = c(1:0, seq(10, 50, by = 10))

rad <- tune(svm, kernel = "radial", data = auto, Y~ ., ranges = list(cost = costs, gamma = gammas))

poly <- tune(svm, kernel = "polynomial", data = auto, Y~ ., ranges = list(cost = costs, degree = 1:7))

list(Linear =auto_svm$performances, Polynomial =poly$performances, Radial = rad$performances) %>% 
  bind_rows(.id = "Type") %>% 
  ggplot(aes(x = log2(cost), y = error, col = Type)) +
  geom_smooth()

rad$performances %>% distinct(gamma, .keep_all = TRUE) %>%  ggplot(aes(x = gamma, y = error)) +
  geom_line()

poly$performances %>% distinct(degree, .keep_all = TRUE) %>%
  ggplot(aes(x = degree, y = error)) +
  geom_line()

map(combn(names(auto), m =2, simplify = FALSE), ~ggplot(auto, aes(x = !!sym(.x[1]), y = !!sym(.x[2]), col = as.factor(Y))) + geom_point())
```

